{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Teleconnection Indices as Predictors for Atmospheric Parameters #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Setup and Data Aggregation/Cleaning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING LIBRARIES\n",
    "\n",
    "# The following libraries are used as part of this project\n",
    "\n",
    "# data modules\n",
    "import pandas as pd\n",
    "from cfgrib.xarray_store import open_dataset\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# data aggregation libraries\n",
    "import ecmwf.data as ecdata\n",
    "from ecmwf.opendata import Client\n",
    "from pathlib import Path\n",
    "\n",
    "# plotting modules\n",
    "from magpye import GeoMap\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plot\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# statistical packages\n",
    "from scipy.signal import savgol_filter\n",
    "import statsmodels.api as sm\n",
    "import xarray.ufuncs as xu\n",
    "import scipy.stats as stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# climatology packages using meteostat\n",
    "from meteostat import Point\n",
    "from meteostat import Stations, Daily, Hourly, Monthly\n",
    "\n",
    "# misc packages\n",
    "from prettytable import PrettyTable\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from os.path import dirname, join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input is provided to initialize model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## User Input To Initialze Model #######################################\n",
    "\n",
    "# Weather parameter input\n",
    "user_input_wx_var = 'surface_temp'\n",
    "\n",
    "# Align variable names to be used by the meteostats network\n",
    "if user_input_wx_var == 'surface_temp':\n",
    "    climo_indice = 'tavg'\n",
    "    units = 'deg. F'\n",
    "\n",
    "if user_input_wx_var == 'surface_wind':\n",
    "    climo_indice = 'wspd'\n",
    "    units = 'mph'\n",
    "\n",
    "if user_input_wx_var == 'surface_pressure':\n",
    "    climo_indice = 'pres'\n",
    "    units = 'hpa'\n",
    "\n",
    "if user_input_wx_var == 'precipitation':\n",
    "    climo_indice = 'prcp' \n",
    "    units = 'mm'\n",
    "\n",
    "# Location input\n",
    "user_input_loc = 'Buffalo'\n",
    "\n",
    "# Season selection\n",
    "season_sel = 'winter'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of city locations using lat/lon identifiers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Initialize and Clean for Analysis #########################################\n",
    "\n",
    "# Dictionary of cities - this includes lat/lon, city name and title for each location\n",
    "\n",
    "cities = {\n",
    "    'Boston': {\n",
    "        'latitude': 42.090925,\n",
    "        'longitude': -71.26435,\n",
    "        'city_name' : 'Boston',\n",
    "        'title': 'Boston, MA',\n",
    "        \n",
    "    },\n",
    "    'Chicago': {\n",
    "        'latitude': 41.862306,\n",
    "        'longitude': -87.616672,\n",
    "        'city_name' : 'Chicago',\n",
    "        'title': 'Chicago, IL',\n",
    "    },\n",
    "    'New York': {\n",
    "        'latitude': 40.812194,\n",
    "        'longitude': -74.076983,\n",
    "        'city_name' : 'New York',\n",
    "        'title': 'New York, NY',\n",
    "    },\n",
    "    'Buffalo': {\n",
    "        'latitude': 42.773739,\n",
    "        'longitude': -78.786978,\n",
    "        'city_name' : 'Buffalo',\n",
    "        'title': 'Buffalo, NY',\n",
    "    },\n",
    "    'Pittsburg': {\n",
    "        'latitude': 40.446786,\n",
    "        'longitude': -80.015761,\n",
    "        'city_name' : 'Pittsburg',\n",
    "        'title': 'Pittsburg, PA',\n",
    "    },\n",
    "    'Philadelphia': {\n",
    "        'latitude': 39.900775,\n",
    "        'longitude': -75.167453,\n",
    "        'city_name' : 'Philadelphia',\n",
    "        'title': 'Philadelphia, PA',\n",
    "    },\n",
    "    'Cleveland': {\n",
    "        'latitude': 41.506022,\n",
    "        'longitude': -81.699564,\n",
    "        'city_name' : 'Cleveland',\n",
    "        'title': 'Cleveland, OH',\n",
    "    },\n",
    "    'Cincinnati': {\n",
    "        'latitude': 39.095442,\n",
    "        'longitude': -84.516039,\n",
    "        'city_name' : 'Cincinnati',\n",
    "        'title': 'Cincinnati, OH',\n",
    "    },\n",
    "    'Washington DC': {\n",
    "        'latitude': 38.907697,\n",
    "        'longitude': -76.864517,\n",
    "        'city_name' : 'Washington DC',\n",
    "        'title': 'Washington DC',\n",
    "    },\n",
    "    'Baltimore': {\n",
    "        'latitude': 39.277969,\n",
    "        'longitude':-76.622767,\n",
    "        'city_name' : 'Baltimore',\n",
    "        'title': 'Baltimore, MD',\n",
    "    },\n",
    "    'Nashville': {\n",
    "        'latitude': 36.166461,\n",
    "        'longitude':-86.771289,\n",
    "        'city_name' : 'Nashville',\n",
    "        'title': 'Nashville, TN',\n",
    "    },\n",
    "    'Charlotte': {\n",
    "        'latitude': 35.225808,\n",
    "        'longitude': -80.852861,\n",
    "        'city_name' : 'Charlotte',\n",
    "        'title': 'Charlotte, NC',\n",
    "    },\n",
    "    'Jacksonville': {\n",
    "        'latitude': 30.323925,\n",
    "        'longitude': -81.637356,\n",
    "        'city_name' : 'Jacksonville',\n",
    "        'title': 'Jacksonville, FL',\n",
    "    },\n",
    "    'Tampa Bay': {\n",
    "        'latitude':27.975967,\n",
    "        'longitude': -82.50335,\n",
    "        'city_name' : 'Tampa Bay',\n",
    "        'title': 'Tampa Bay, FL',\n",
    "    },\n",
    "    'Miami': {\n",
    "        'latitude': 25.957919,\n",
    "        'longitude': -80.238842,\n",
    "        'city_name' : 'Miami',\n",
    "        'title': 'Miami, FL',\n",
    "    },\n",
    "    'Green Bay': {\n",
    "        'latitude': 44.501306,\n",
    "        'longitude': -88.062167,\n",
    "        'city_name' : 'Green Bay',\n",
    "        'title': 'Green Bay, WI',\n",
    "    },\n",
    "    'Kansas City': {\n",
    "        'latitude': 39.048914,\n",
    "        'longitude': -94.484039,\n",
    "        'city_name' : 'Kansas City',\n",
    "        'title': 'Kansas City, MO',\n",
    "    },\n",
    "    'Denver': {\n",
    "        'latitude': 39.743936,\n",
    "        'longitude': -105.020097,\n",
    "        'city_name' : 'Denver',\n",
    "        'title': 'Denver, CO',\n",
    "    },\n",
    "    'Seattle': {\n",
    "        'latitude': 47.595153,\n",
    "        'longitude': -122.331625,\n",
    "        'city_name' : 'Seattle',\n",
    "        'title': 'Seattle, WA',\n",
    "    },\n",
    "    'San Francisco': {\n",
    "        'latitude': 37.713486,\n",
    "        'longitude':-122.386256,\n",
    "        'city_name' : 'San Francisco',\n",
    "        'title': 'San Francisco, CA',\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Aggregation and Cleaning ##\n",
    "#### ESRL PSL monthly teleconnection data is extracted via a url for the ENSO, PDO, AO, NAO, and PNA. This data will be used as part of a multivariate linear analysis to determine predictability of atmospheric variables using these indices over specified locations. The data is first downloaded using the provided url and then saved as a Pandas data frame. The data is then vertically orientated and stacked so the data of the monthly indice becomes the index. This is completed for each of the 5 teleconnection indices. Data is then merged using an outer join technique using the “Date” index to join the different indices together. Missing values are then converted to 'NaN' values. All row values for each column are dropped if a NaN is detected in that row using the 'dropna' function. ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load teleconnection datasets from ESRL PSL for ENSO, PDO, NAO, AO, and PNA\n",
    "\n",
    "enso = pd.read_csv('https://www.esrl.noaa.gov/psd/data/correlation/censo.data',delim_whitespace=True,header=None,skiprows=1,skipfooter=2, engine='python')\n",
    "pdo = pd.read_csv('https://www.esrl.noaa.gov/psd/data/correlation/pdo.data',delim_whitespace=True,header=None,skiprows=1,skipfooter=14, engine='python')\n",
    "nao = pd.read_csv('https://www.esrl.noaa.gov/psd/data/correlation/nao.data',delim_whitespace=True,header=None,skiprows=1,skipfooter=3, engine='python')\n",
    "ao = pd.read_csv('https://www.esrl.noaa.gov/psd/data/correlation/ao.data',delim_whitespace=True,header=None,skiprows=1,skipfooter=3, engine='python')\n",
    "pna = pd.read_csv('https://www.esrl.noaa.gov/psd/data/correlation/pna.data',delim_whitespace=True,header=None,skiprows=1,skipfooter=3, engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize and clean data - ENSO\n",
    "\n",
    "# create new date column\n",
    "enso_new=pd.DataFrame()\n",
    "\n",
    "# create date_ranage - fequency by month\n",
    "enso_new['Date']=pd.date_range(start=pd.datetime(1948,1,1),end=pd.datetime(2022,12,1),freq=\"MS\")\n",
    "\n",
    "# set index as date column\n",
    "enso_new = enso_new.set_index('Date')\n",
    "\n",
    "# stack so values are vertically integrated for each date\n",
    "len(enso.loc[:,1:].stack().values)\n",
    "enso_new['ENSO']=enso.loc[:,1:].stack().values\n",
    "\n",
    "# only select time frame extending from 1950 to 2021 due to data sample compatability\n",
    "enso_new = enso_new.loc[(enso_new.index.year >= 1950) & (enso_new.index.year <=2021)]\n",
    "\n",
    "# print dataframe\n",
    "enso_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above steps for each teleconnection indice (NAO, PDO, AO, PNA)\n",
    "\n",
    "# set empy pandas dataframes\n",
    "nao_new=pd.DataFrame()\n",
    "pdo_new=pd.DataFrame()\n",
    "ao_new=pd.DataFrame()\n",
    "pna_new=pd.DataFrame()\n",
    "\n",
    "# Identify data range to accomodate full list of indices\n",
    "nao_new['Date'] = pd.date_range(start=pd.datetime(1948,1,1),end=pd.datetime(2022,12,1),freq=\"MS\")\n",
    "pdo_new['Date'] = pd.date_range(start=pd.datetime(1948,1,1),end=pd.datetime(2022,12,1),freq=\"MS\")\n",
    "ao_new['Date'] = pd.date_range(start=pd.datetime(1950,1,1),end=pd.datetime(2022,12,1),freq=\"MS\")\n",
    "pna_new['Date'] = pd.date_range(start=pd.datetime(1948,1,1),end=pd.datetime(2022,12,1),freq=\"MS\")\n",
    "\n",
    "# set index as 'Date'\n",
    "nao_new = nao_new.set_index('Date')\n",
    "pdo_new = pdo_new.set_index('Date')\n",
    "ao_new = ao_new.set_index('Date')\n",
    "pna_new = pna_new.set_index('Date')\n",
    "\n",
    "# vertically stack indices to correspond with Date index\n",
    "nao_new['NAO']=nao.loc[:,1:].stack().values\n",
    "pdo_new['PDO']=pdo.loc[:,1:].stack().values\n",
    "ao_new['AO']=ao.loc[:,1:].stack().values\n",
    "pna_new['PNA']=pna.loc[:,1:].stack().values\n",
    "\n",
    "# only select time frame extending from 1950 to 2021 due to data sample compatability\n",
    "nao_new = nao_new.loc[(nao_new.index.year >= 1950) & (nao_new.index.year <=2021)]\n",
    "pdo_new = pdo_new.loc[(pdo_new.index.year >= 1950) & (pdo_new.index.year <=2021)]\n",
    "ao_new = ao_new.loc[(ao_new.index.year >= 1950) & (ao_new.index.year <=2021)]\n",
    "pna_new = pna_new.loc[(pna_new.index.year >= 1950) & (pna_new.index.year <=2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is merged using each teleconnection indice. An outer join is performed on the date column\n",
    "newdf_all = pd.merge(enso_new,pdo_new, how= 'outer',on='Date')\n",
    "newdf_all = pd.merge(newdf_all,nao_new, how= 'outer',on='Date')\n",
    "newdf_all = pd.merge(newdf_all,ao_new, how= 'outer',on='Date')\n",
    "newdf_all = pd.merge(newdf_all,pna_new, how= 'outer',on='Date')\n",
    "\n",
    "# set nans for values that are numerically indicated as missing\n",
    "newdf_all['PDO'][newdf_all['PDO'] <= -9.9] = np.nan \n",
    "newdf_all['ENSO'][newdf_all['ENSO'] <= -9.9] = np.nan \n",
    "newdf_all['NAO'][newdf_all['NAO'] <= -9.9] = np.nan\n",
    "newdf_all['AO'][newdf_all['AO'] <= -9.9] = np.nan\n",
    "newdf_all['PNA'][newdf_all['PNA'] <= -9.9] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIMATOLOGY CALCULATIONS ###\n",
    "\n",
    "#### a full climotology is calculated using the Meteostat Python Package. Nearest station approximator is used to find observational stations closest to specified lat/lon location. A station is located that is closest, but also goes as far back as possible, preferably back to 1948 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full climo for station location extending 1950-2021\n",
    "# This will need to be customized based on availablility of station location data\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1950, 1, 1)\n",
    "end = datetime(2021, 12, 31)\n",
    "\n",
    "# nearby stations are aggregated \n",
    "stations = Stations()\n",
    "stations = stations.nearby(cities[user_input_loc]['latitude'],cities[user_input_loc]['longitude'])\n",
    "\n",
    "# 10 of the closest observation stations are used to determine which station best meets date criteria. \n",
    "station = stations.fetch(10)\n",
    "print(\"The stations for the locations included:\", station)\n",
    "for a in range(0,10,1):   \n",
    "        if (station['monthly_start'].dt.year.values[a] <= 1950) & (station['monthly_end'].dt.year.values[a] >= 2021):\n",
    "            print(\"this station works\")\n",
    "            station_climo = station.loc[(station.index== station.index[a])]\n",
    "            break\n",
    "\n",
    "station = station_climo.index.values[0]\n",
    "\n",
    "# Get monthly data\n",
    "data_monthly_climo = Monthly(station, start, end)\n",
    "data_monthly_climo = data_monthly_climo.fetch()\n",
    "\n",
    "# index is set to \"Date\" so data can be merged to existing teleconnection DF\n",
    "data_monthly_climo.index.names = ['Date']\n",
    "\n",
    "if climo_indice == 'tavg':\n",
    "    data_monthly_climo.tavg = (data_monthly_climo .tavg * 9/5) + 32\n",
    "\n",
    "if climo_indice == 'wspd':\n",
    "    data_monthly_climo.wspd = (data_monthly_climo.wspd/1.609)\n",
    "\n",
    "# only relavent weather data (as specified by the user), is extracted\n",
    "var_monthly_climo = data_monthly_climo[climo_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteostat station data is merged to teleconnection DF \n",
    "\n",
    "newdf_climo = pd.merge(newdf_all,data_monthly_climo[climo_indice], how= 'outer',on='Date')\n",
    "\n",
    "# dataset with all met data and teleconnection indices\n",
    "newdf_climo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure to clean dataset of nans - this is completed for all columns\n",
    "\n",
    "# drop nan values over all columns\n",
    "newdf_climo_imp = newdf_climo.dropna()\n",
    "\n",
    "# confirm all nan values have been removed\n",
    "print(newdf_climo_imp.isna().sum())\n",
    "\n",
    "# refined dataset by removing NaNs\n",
    "newdf_climo_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal data aggregation - only months selected for the identified season will be considered\n",
    "\n",
    "if season_sel == 'winter':\n",
    "    season_data = newdf_climo_imp.loc[(newdf_climo_imp.index.month.values == 12) |\n",
    "                                        (newdf_climo_imp.index.month.values == 1) |\n",
    "                                        (newdf_climo_imp.index.month.values == 2)]\n",
    "if season_sel == 'spring':\n",
    "    season_data = newdf_climo_imp.loc[(newdf_climo_imp.index.month.values == 3) |\n",
    "                                        (newdf_climo_imp.index.month.values == 4) |\n",
    "                                        (newdf_climo_imp.index.month.values == 5)]\n",
    "if season_sel == 'summer':\n",
    "    season_data = newdf_climo_imp.loc[(newdf_climo_imp.index.month.values == 6) |\n",
    "                                        (newdf_climo_imp.index.month.values == 7) |\n",
    "                                        (newdf_climo_imp.index.month.values == 8)]\n",
    "if season_sel == 'fall':\n",
    "    season_data = newdf_climo_imp.loc[(newdf_climo_imp.index.month.values == 9) |\n",
    "                                        (newdf_climo_imp.index.month.values == 10) |\n",
    "                                        (newdf_climo_imp.index.month.values == 11)]\n",
    "newdf_climo_imp = season_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Pairplot ##########\n",
    "sns.pairplot(newdf_climo_imp)\n",
    "\n",
    "\n",
    "######### Heat Map ##########\n",
    "\n",
    "# correlate values in teleconnection and atmos variable dataset\n",
    "newdf_climo_corr = newdf_climo_imp.corr() \n",
    "\n",
    "# mask out upper triangle of values                                             \n",
    "mask = np.triu(np.ones_like(newdf_climo_corr, dtype=bool))\n",
    "\n",
    "# initiate subplot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "cmap = 'coolwarm' # color map to identify +/- correlations\n",
    "\n",
    "# create heatmap of correlation values                                 \n",
    "sns.heatmap(newdf_climo_corr, mask=mask,annot=True, fmt=\".2f\", # two digits after decimal point\n",
    "          cmap=cmap, vmin=-1, vmax=1, # (3) \n",
    "           square=True,linewidth=1)  # (4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Technique 1: Multivariate Linear Regression Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up target and features datasets\n",
    "\n",
    "# X_data includes the teleconnection indices which is our features dataset\n",
    "X_data= newdf_climo_imp.drop(climo_indice, axis=1) \n",
    "\n",
    "# y_data is our target dataset. This is the met variable specified by the user\n",
    "y_data = newdf_climo_imp[climo_indice]\n",
    "\n",
    "# testing and training dataset are split using a 30%/70% split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_data, y_data,\n",
    "                                                test_size=0.7, train_size=0.3) # 70/30 split used for training and testing\n",
    "\n",
    "# initiate and run linear model\n",
    "\n",
    "model = LinearRegression() # instantiate model\n",
    "model.fit(Xtrain, ytrain)  # fit model to data - needs to be applied to training data\n",
    "y_model = model.predict(Xtest) # use model to predict target variable (y) using set of feature variables (x)\n",
    "\n",
    "# testing meterics\n",
    "r2_scores = model.score(Xtrain, ytrain) # R^2 value\n",
    "print('r2 (coef of determination) score is',r2_scores) \n",
    "print('mean_sqrd_error is==',mean_squared_error(ytest,y_model)) # find RMSE between target testing and modeled dataset\n",
    "print('root_mean_squared error of is==',np.sqrt(mean_squared_error(ytest,y_model))) # print score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Model Performance Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats Summary:\n",
    "\n",
    "# Alternative method using Statsmodel package to retrieve stats summary\n",
    "\n",
    "#define response variable\n",
    "y = newdf_climo_imp[climo_indice]\n",
    "\n",
    "#define predictor variables\n",
    "x = newdf_climo_imp.drop(climo_indice, axis=1)\n",
    "\n",
    "#add constant to predictor variables\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#fit linear regression model\n",
    "model_v2 = sm.OLS(y, x).fit()\n",
    "\n",
    "#view model summary\n",
    "print(model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Observed Vs. Predicted Values\n",
    "\n",
    "# scatter and best fit using target testing dataset and linear model calibrated using the features testing dataset\n",
    "ytest_plot = ytest.reset_index(drop=True)\n",
    "fig,ax=plt.subplots(figsize=(20,10))\n",
    "plt.grid()\n",
    "plt.scatter(ytest_plot,y_model)\n",
    "plt.title(\"Scatter Plot and Best-Fit of Observed vs. Predicted {}\".format(climo_indice))\n",
    "plt.xlabel('Observed {} ({})'.format(climo_indice, units))\n",
    "plt.ylabel('Predicted {} ({})'.format(climo_indice, units))\n",
    "lim = plt.axis()\n",
    "\n",
    "# best fit using polyfit function\n",
    "m, b = np.polyfit(ytest_plot,y_model, 1)\n",
    "plt.plot(ytest_plot, m*ytest_plot+b, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals Analysis\n",
    "\n",
    "# Plot residuals - scatterplot\n",
    "fig,ax=plt.subplots()\n",
    "ax.scatter(y_model,ytest-y_model,color='b')\n",
    "ax.set_xlabel('Predicted {} ({})'.format(climo_indice, units))\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Scatter Plot of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals - histogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(ytest-y_model)\n",
    "ax.set_xlabel('Residual')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Technique 2: Random Forest Regression Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression Model Initiation\n",
    "\n",
    "# set grid with hyperparameters\n",
    "rf_grid ={'bootstrap': [True, False], # method for sampling data points (with/without replacement)  \n",
    "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],  # num. of levels in each descision tree\n",
    "            'max_features': ['auto', 'sqrt'],  # max number of features for splitting a node\n",
    "            'min_samples_leaf': [1, 2, 4],  # min num. of points allowed in a leaf node\n",
    "            'min_samples_split': [2, 5, 10], # min number of data points placed in a node before the node is split \n",
    "            'n_estimators': [50, 100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]} # num. of trees in the forest\n",
    "\n",
    "# identify model\n",
    "rf_base = RandomForestRegressor()\n",
    "\n",
    "# perform grid search with cross validation score\n",
    "rf_random = RandomizedSearchCV(estimator=rf_base, param_distributions=rf_grid, cv=7) # k-fold cross validation = 7\n",
    "\n",
    "# fit the model using training data\n",
    "regress_tree_output = rf_random.fit(Xtrain, ytrain)\n",
    "\n",
    "regress_tree_output # output printed\n",
    "\n",
    "# identify best model based on hyperparameter input\n",
    "model_rf = rf_random.best_estimator_ \n",
    "print(model_rf)\n",
    "\n",
    "# cross validation scores using identified model\n",
    "score_rf = cross_val_score(model_rf, Xtrain, ytrain, cv=7)\n",
    "print(score_rf)\n",
    "score_mean_rf = score_rf.mean()\n",
    "print(score_mean_rf) # mean score of model performance output\n",
    "\n",
    "model_rf.fit(Xtrain, ytrain)\n",
    "X_train_preds = model_rf.predict(Xtrain)\n",
    "X_test_preds = model_rf.predict(Xtest)\n",
    "\n",
    "# mean square/square root and R^2 values\n",
    "print('train r2: {}'.format(r2_score(ytrain, X_train_preds)))\n",
    "print('train mse: {}'.format(mean_squared_error(ytrain, X_train_preds)))\n",
    "print('train rmse: {}'.format(sqrt(mean_squared_error(ytrain, X_train_preds))))\n",
    "\n",
    "print()\n",
    "print('test r2: {}'.format(r2_score(ytest, X_test_preds)))\n",
    "print('test mse: {}'.format(mean_squared_error(ytest, X_test_preds)))\n",
    "print('test rmse: {}'.format(sqrt(mean_squared_error(ytest, X_test_preds))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression Model Performance Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Observed Vs. Predicted using Random Forest Regressor\n",
    "\n",
    "# scatter and best fit using target testing dataset and linear model calibrated using the features testing dataset\n",
    "#ytest_plot = ytest.reset_index(drop=True)\n",
    "fig,ax=plt.subplots(figsize=(20,10))\n",
    "plt.grid()\n",
    "plt.scatter(ytest, X_test_preds)\n",
    "plt.title(\"Scatter Plot and Best-Fit of Observed vs. Predicted {}\".format(climo_indice))\n",
    "plt.xlabel('Observed {} ({})'.format(climo_indice, units))\n",
    "plt.ylabel('Predicted {} ({})'.format(climo_indice, units))\n",
    "lim = plt.axis()\n",
    "\n",
    "# best fit using polyfit function\n",
    "m, b = np.polyfit(ytest,X_test_preds, 1)\n",
    "plt.plot(ytest, m*ytest+b, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Importance Analysis\n",
    "# The feature importance determines which features as part of the training dataset are most important/valiable as predictors for the \n",
    "# target dataset\n",
    "\n",
    "# feature_importances_ gives the importance of each feature in the order in which the features are arranged in training dataset.\n",
    "\n",
    "# it is important to keep in mind that the importance of these features can become inflated artifically - indicating a false high\n",
    "# predictive rating as a result of overfitting.\n",
    "\n",
    "feature_names = list(X_data.columns) #rf[:-1].get_feature_names_out()\n",
    "\n",
    "mdi_importances = pd.Series(\n",
    "    model_rf.feature_importances_, index=feature_names\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "#feature_names = model_rf[:-1].get_feature_names_out()\n",
    "\n",
    "# plot to show feature importance\n",
    "ax = mdi_importances.plot.barh()\n",
    "ax.set_title(\"Random Forest Feature Importances\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance Analysis\n",
    "# this is defined as the decrease in a model score when a single feature value is randomly shuffled. \n",
    "# This procedure breaks the relationship between the feature and the target, thus the drop in the model \n",
    "# score is indicative of how much the model depends on the feature. This test is also not bias toward high-cardinality features\n",
    "\n",
    "result = permutation_importance(\n",
    "    model_rf,Xtest, ytest, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_data.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
